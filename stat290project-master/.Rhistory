#'
#' @description Given a dataframe produced by the generate_data function, this function does model
#' fitting and returns a fit.
#'
#' @param df dataframe generated by the generate_data function
#' @param modelfn modeling function to be used
#' @return the fit of the modeling function
#'
#' @importFrom stats lm
#' @importFrom randomForest randomForest
#' @importFrom gbm gbm
#'
#' @export
#'
model_fitting <- function(df) {
idx = sample(1:dim(df)[1], round(2*dim(df)[1]/3))
train = df[idx,]
test = df[-idx,]
# out.test = test[,1];
# in.test = test[,-1];
# out.train = train[,1];
# in.train = train[,-1]
# return(test[,-1])
f <- as.formula(paste('train[,1] ~', paste(colnames(train)[2:dim(df)[2]], collapse='+')))
#modelAllHexSubscales <- lm(f, HHdata)
fit.lm <- lm(f,train)
pred.lm = predict(fit.lm,test)
plot(test[,1],pred.lm); abline(0,1);
summary(fit)
fit.rf = randomForest(f,train,importance = TRUE)
pred.rf = predict(fit.rf,test)
plot(test[,1],pred.rf); abline(0,1);
importance(fit.rf)
fit.bst = gbm(f,data = train, n.trees = 500, interaction.depth = 4)
pred.bst = predict(fit.bst,newdata = test,n.trees = 500)
plot(test[,1],pred.bst); abline(0,1);
summary(fit.bst)
}
generate(2,500)
generate_data(2,500)
#' Input data and use the given model fitting algorithm. One of 3 - lm, boosting, random forest.
#' Default is lm
#'
#' @description Given a dataframe produced by the generate_data function, this function does model
#' fitting and returns a fit.
#'
#' @param df dataframe generated by the generate_data function
#' @param modelfn modeling function to be used
#' @return the fit of the modeling function
#'
#' @importFrom stats lm
#' @importFrom randomForest randomForest
#' @importFrom gbm gbm
#'
#' @export
#'
model_fitting <- function(df) {
idx = sample(1:dim(df)[1], round(2*dim(df)[1]/3))
train = df[idx,]
test = df[-idx,]
# out.test = test[,1];
# in.test = test[,-1];
# out.train = train[,1];
# in.train = train[,-1]
# return(test[,-1])
f <- as.formula(paste('train[,1] ~', paste(colnames(train)[2:dim(df)[2]], collapse='+')))
#modelAllHexSubscales <- lm(f, HHdata)
fit.lm <- lm(f,train)
pred.lm = predict(fit.lm,test)
plot(test[,1],pred.lm); abline(0,1);
summary(fit.lm)
fit.rf = randomForest(f,train,importance = TRUE)
pred.rf = predict(fit.rf,test)
plot(test[,1],pred.rf); abline(0,1);
importance(fit.rf)
fit.bst = gbm(f,data = train, n.trees = 500, interaction.depth = 4)
pred.bst = predict(fit.bst,newdata = test,n.trees = 500)
plot(test[,1],pred.bst); abline(0,1);
summary(fit.bst)
}
generate(2,500)
generate_data(2,500)
library(deSolve)
?radau
knitr::opts_chunk$set(echo = TRUE)
# load data
load("body.RData")
# training and test sets
set.seed(1)
test = sample(1:dim(X)[1], 200)
train = -test
# Bagging and random forests
library(ranger)
testMSE.bag = numeric(0);testMSE.rf = numeric(0)
for (i in 1:150){
fit.bag = ranger(Y$Weight[train]~.,data = X[train,], mtry = dim(X)[2], num.trees = i*20)
pred.bag = predict(fit.bag, X[test,])
testMSE.bag[i] = mean((Y$Weight[test] - pred.bag$predictions)^2)
fit.rf = ranger(Y$Weight[train]~.,data = X[train,], mtry = round(dim(X)[2]/3), num.trees = i*10)
pred.rf = predict(fit.rf, X[test,])
testMSE.rf[i] = mean((Y$Weight[test] - pred.rf$predictions)^2)
}
setwd("C:/Users/pdutta03/Google Drive/Q-Winter2017/STATS 216/Assgn4")
plot(x = 10*(1:i),y = testMSE.rf,type="l",col = "red",xlab = "number of trees",ylab = "testMSE")
lines(x = 10*(1:i),y = testMSE.bag,type="l",col = "green")
legend("topright",legend=c("Random forest", "Bagging"),col=c("red", "green"), lty=1:2, cex=0.8)
fit.bag2 = ranger(Y$Weight[train]~.,data = X[train,], mtry = dim(X)[2], num.trees = 1000, importance ='impurity')
sort(importance(fit.bag2),decreasing = TRUE)
fit.rf2 = ranger(Y$Weight[train]~.,data = X[train,], mtry = sort(dim(X)[2]/3), num.trees = 500, importance ='impurity')
sort(importance(fit.rf2),decreasing = TRUE)
fit.bag2 = ranger(Y$Weight[train]~.,data = X[train,], mtry = dim(X)[2], num.trees = 500, importance ='impurity')
sort(importance(fit.bag2),decreasing = TRUE)
fit.bag2 = ranger(Y$Weight[train]~.,data = X[train,], mtry = dim(X)[2], num.trees = 700, importance ='impurity')
sort(importance(fit.bag2),decreasing = TRUE)
# load data
load("body.RData")
# training and test sets
set.seed(1)
test = sample(1:dim(X)[1], 200)
train = -test
# Bagging and random forests
library(ranger)
testMSE.bag = numeric(0);testMSE.rf = numeric(0)
for (i in 1:150){
fit.bag = ranger(Y$Weight[train]~.,data = X[train,], mtry = dim(X)[2], num.trees = i*20)
pred.bag = predict(fit.bag, X[test,])
testMSE.bag[i] = mean((Y$Weight[test] - pred.bag$predictions)^2)
fit.rf = ranger(Y$Weight[train]~.,data = X[train,], mtry = round(dim(X)[2]/3), num.trees = i*20)
pred.rf = predict(fit.rf, X[test,])
testMSE.rf[i] = mean((Y$Weight[test] - pred.rf$predictions)^2)
}
plot(x = 20*(1:i),y = testMSE.rf,type="l",col = "red",xlab = "number of trees",ylab = "testMSE")
lines(x = 20*(1:i),y = testMSE.bag,type="l",col = "green")
legend("topright",legend=c("Random forest", "Bagging"),col=c("red", "green"), lty=1:2, cex=0.8)
plot(x = 20*(1:i),y = testMSE.bag,type="l",col = "red",xlab = "number of trees",ylab = "testMSE")
lines(x = 20*(1:i),y = testMSE.rf,type="l",col = "green")
legend("topright",legend=c("Bagging", "Random Forest"),col=c("red", "green"), lty=1:2, cex=0.8)
?plot
min(testMSE.rf)
max(testMSE.bag)
plot(x = 20*(1:i),y = testMSE.bag,type="l",col = "red",xlab = "number of trees",ylab = "testMSE",ylim = c(9,13))
lines(x = 20*(1:i),y = testMSE.rf,type="l",col = "green")
legend("topright",legend=c("Bagging", "Random Forest"),col=c("red", "green"), lty=1:2, cex=0.8)
plot(x = 20*(1:i),y = testMSE.bag,type="l",col = "red",xlab = "number of trees",ylab = "testMSE",ylim = c(9.5,13))
lines(x = 20*(1:i),y = testMSE.rf,type="l",col = "green")
legend("topright",legend=c("Bagging", "Random Forest"),col=c("red", "green"), lty=1:2, cex=0.8)
fit.bag2 = ranger(Y$Weight[train]~.,data = X[train,], mtry = dim(X)[2], num.trees = 1000, importance ='impurity')
sort(importance(fit.bag2),decreasing = TRUE)
fit.bag2 = ranger(Y$Weight[train]~.,data = X[train,], mtry = dim(X)[2], num.trees = 500, importance ='impurity')
sort(importance(fit.bag2),decreasing = TRUE)
fit.rf2 = ranger(Y$Weight[train]~.,data = X[train,], mtry = sort(dim(X)[2]/3), num.trees = 500, importance ='impurity')
sort(importance(fit.rf2),decreasing = TRUE)
?ranger
fit.rf3 = ranger(Y$Weight[train]~.,data = X[train,], mtry = round(dim(X)[2]/3),num.trees =500)
pred.rf3 = predict(fit.rf3, X[test,])
testMSE.rf3 = sqrt(mean((Y$Weight[test] - pred.rf3$predictions)^2))
set.seed(1)
fit.rf3 = ranger(Y$Weight[train]~.,data = X[train,], mtry = round(dim(X)[2]/3),num.trees =500)
pred.rf3 = predict(fit.rf3, X[test,])
testMSE.rf3 = sqrt(mean((Y$Weight[test] - pred.rf3$predictions)^2))
set.seed(1)
fit.rf3 = ranger(Y$Weight[train]~.,data = X[train,], mtry = round(dim(X)[2]/3),num.trees =500)
pred.rf3 = predict(fit.rf3, X[test,])
testMSE.rf3 = sqrt(mean((Y$Weight[test] - pred.rf3$predictions)^2))
set.seed(1)
fit.rf3 = ranger(Y$Weight[train]~.,data = X[train,], mtry = round(dim(X)[2]/3),num.trees =500)
pred.rf3 = predict(fit.rf3, X[test,])
testMSE.rf3 = sqrt(mean((Y$Weight[test] - pred.rf3$predictions)^2))
X1 = c(5.30,3.30,7.30,1.30,3.30,7.30,7.30)
X2 = c(7.30,3.30,7.30,7.30,1.3,5.3,1.30)
Y = c("green","green","green","green","red","red","red")
plot(X1,X2,col=Y,pch = 19)
library(e1071)
X = data.frame(X1 = X1,X2 = X2)
fit.mmc = svm(as.factor(Y)~.,data = X, kernel = "linear", cost = 10, scale = F)
beta=drop(t(fit.mmc$coefs)%*%as.matrix(X[fit.mmc$index,]))
beta0=fit.mmc$rho
plot(X1,X2,col=Y,pch = 19);
abline(beta0/beta[2],-beta[1]/beta[2])
suppressMessages(library(e1071))
X = data.frame(X1 = X1,X2 = X2)
fit.mmc = svm(as.factor(Y)~.,data = X, kernel = "linear", cost = 10, scale = F)
beta=drop(t(fit.mmc$coefs)%*%as.matrix(X[fit.mmc$index,]))
beta0=fit.mmc$rho
plot(X1,X2,col=Y,pch = 19);
abline(beta0/beta[2],-beta[1]/beta[2])
sprintf("Equation of the optimal hyperplane is given by:  %s + %s X1 + X2  = 0",-beta0/beta[2],beta[1]/beta[2])
?round
sprintf("Equation of the optimal hyperplane is given by:  %s + %s X1 + X2  = 0",round(-beta0/beta[2],digits = 3),round(beta[1]/beta[2],digits = 3))
sprintf("Classify to Red if %s + %s X1 + X2  < 0, and classify to Green otherwise.", round(-beta0/beta[2],digits = 3),round(beta[1]/beta[2],digits = 3))
plot(X1,X2,col=Y,pch = 19);
abline(beta0/beta[2],-beta[1]/beta[2])
abline((beta0-1)/beta[2],-beta[1]/beta[2],lty = 2)
abline((beta0+1)/beta[2],-beta[1]/beta[2],lty = 2)
mwd = sqrt(2)
sprintf("Width of margin: %s", mwd)
fit.mmc$SV
plot(X1,X2,col=Y,pch = 19);
abline(beta0/beta[2],-(beta[1]+0.1)/beta[2])
sprintf("Equation of the non-optimal hyperplane is given by:  %s + %s X1 + X2  = 0",-beta0/beta[2],(beta[1]+0.1)/beta[2])
plot(c(5.29,3.30,7.30,1.28,3.32,7.30,7.29,3),c(7.30,3.29,7.29,7.30,1.29,5.31,1.30,6),col = c("green","green","green","green","red","red","red","red",pch = 19),xlab = "X1",ylab = "X2")
points(3,6,pch=5,cex=2)
library(ISLR)
library(e1071)
OJ$STORE = as.factor(OJ$STORE)
OJ$StoreID = as.factor(OJ$StoreID)
set.seed(1)
idx = sample(1:dim(OJ)[1],535)
train = OJ[idx,]
test = OJ[-idx,]
fit.svc = svm(train$Purchase~.,data = train, kernel = "linear", cost = 0.05, scale = T)
summary(fit.svc)
train.pred = predict(fit.svc,train)
train.error = table(predict = train.pred,truth = train$Purchase)
train.error
train.pred = predict(fit.svc,train)
train.error = table(predict = train.pred,truth = train$Purchase)
train.error
sprintf("Overall training misclassification rate:  %s",sum(train.error[1,2]+train.error[2,1])/sum(train.error))
test.pred = predict(fit.svc,test)
test.error = table(predict = test.pred,truth = test$Purchase)
test.error
sprintf("Overall test misclassification rate:  %s",sum(test.error[1,2]+test.error[2,1])/sum(test.error))
set.seed (1)
tune.out=tune(svm,Purchase~.,data = train, kernel ="linear",ranges =list(cost=10^seq(-2,1,length.out = 20)))
summary(tune.out)
sprintf("Optimal cost :  %s",tune.out$best.parameters$cost)
### (e) Compute the training and test error rates using this new value for cost.
```{r}
set.seed (1)
tune.out=tune(svm,Purchase~.,data = train, kernel ="linear",ranges =list(cost=10^seq(-2,1,length.out = 20)))
summary(tune.out)
sprintf("Optimal cost :  %s",tune.out$best.parameters$cost)
train.pred2 = predict(tune.out$best.model,newdata = train)
train.error2 = table(predict = train.pred2,truth = train$Purchase)
train.error2
sprintf("Overall training misclassification rate for cost %s :  %s",tune.out$best.parameters$cost,sum(train.error2[1,2]+train.error2[2,1])/sum(train.error2))
train.pred2 = predict(tune.out$best.model,newdata = train)
train.error2 = table(predict = train.pred2,truth = train$Purchase)
train.error2
sprintf("Overall training misclassification rate for linear SVM :  %s",sum(train.error2[1,2]+train.error2[2,1])/sum(train.error2))
test.pred2 = predict(tune.out$best.model,newdata = test)
test.error2 = table(predict = test.pred2,truth = test$Purchase)
test.error2
sprintf("Overall test misclassification rate for linear SVM :  %s",sum(test.error2[1,2]+test.error2[2,1])/sum(test.error2))
fit.svc2 = svm(train$Purchase~.,data = train, kernel = "radial", cost = 0.05, scale = T)
summary(fit.svc2)
fit.svcr = svm(train$Purchase~.,data = train, kernel = "radial", cost = 0.05, scale = T)
summary(fit.svcr)
train.predr = predict(fit.svcr,train)
train.errorr = table(predict = train.predr,truth = train$Purchase)
test.predr = predict(fit.svcr,test)
test.errorr = table(predict = test.predr,truth = test$Purchase)
train.errorr
sprintf("Overall training misclassification rate for radial SVM:  %s",sum(train.errorr[1,2]+train.errorr[2,1])/sum(train.errorr))
test.errorr
sprintf("Overall test misclassification rate for radial SVM:  %s",sum(test.errorr[1,2]+test.errorr[2,1])/sum(test.errorr))
tune.out2=tune(svm,Purchase~.,data = train, kernel ="radial",ranges =list(cost=10^seq(-2,1,length.out = 20)))
summary(tune.out2)
sprintf("Optimal cost for radial SVM:  %s",tune.out2$best.parameters$cost)
train.predr2 = predict(tune.out2$best.model,newdata = train)
train.errorr2 = table(predict = train.predr2,truth = train$Purchase)
train.errorr2
sprintf("Overall training misclassification rate for radial SVM (optimal cost) :  %s",sum(train.errorr2[1,2]+train.errorr2[2,1])/sum(train.errorr2))
train.predr2 = predict(tune.out2$best.model,newdata = train)
train.errorr2 = table(predict = train.predr2,truth = train$Purchase)
test.predr2 = predict(tune.out2$best.model,newdata = test)
test.errorr2 = table(predict = test.predr2,truth = test$Purchase)
train.errorr2
sprintf("Overall training misclassification rate for radial SVM (optimal cost) :  %s",sum(train.errorr2[1,2]+train.errorr2[2,1])/sum(train.errorr2))
test.errorr2
sprintf("Overall test misclassification rate for radial SVM (optimal cost) :  %s",sum(test.errorr2[1,2]+test.errorr2[2,1])/sum(test.errorr2))
fit.svcp = svm(train$Purchase~.,data = train, kernel = "polynomial",degree = 2, cost = 0.05, scale = T)
summary(fit.svcp)
train.predp = predict(fit.svcp,train)
train.errorp = table(predict = train.predp,truth = train$Purchase)
test.predp = predict(fit.svcp,test)
test.errorp = table(predict = test.predp,truth = test$Purchase)
train.errorp
sprintf("Overall training misclassification rate for polynomial kernel (D=2) SVM:  %s",sum(train.errorp[1,2]+train.errorp[2,1])/sum(train.errorp))
test.errorp
sprintf("Overall test misclassification rate for polynomial kernel (D=2) SVM:  %s",sum(test.errorp[1,2]+test.errorp[2,1])/sum(test.errorp))
tune.out3=tune(svm,Purchase~.,data = train, kernel ="polynomial",degree = 2,ranges =list(cost=seq(0.01,10,length.out = 50)))
tune.out3=tune(svm,Purchase~.,data = train, kernel ="polynomial",degree = 2,ranges =list(cost=10^seq(-2,1,length.out = 20)))
summary(tune.out3)
sprintf("Optimal cost for polynomial kernel (D=2) SVM:  %s",tune.out3$best.parameters$cost)
train.predp2 = predict(tune.out3$best.model,newdata = train)
train.errorp2 = table(predict = train.predp2,truth = train$Purchase)
test.predp2 = predict(tune.out3$best.model,newdata = test)
test.errorp2 = table(predict = test.predp2,truth = test$Purchase)
train.errorp2
sprintf("Overall training misclassification rate for polynomial kernel (D=2) SVM (optimal cost) :  %s",tune.out3$best.parameters$cost,sum(train.errorp2[1,2]+train.errorp2[2,1])/sum(train.errorp2))
test.errorp2
sprintf("Overall test misclassification rate for polynomial kernel (D=2) SVM (optimal cost) :  %s",tune.out3$best.parameters$cost,sum(test.errorp2[1,2]+test.errorp2[2,1])/sum(test.errorp2))
train.errorp2
sprintf("Overall training misclassification rate for polynomial kernel (D=2) SVM (optimal cost) :  %s",sum(train.errorp2[1,2]+train.errorp2[2,1])/sum(train.errorp2))
test.errorp2
sprintf("Overall test misclassification rate for polynomial kernel (D=2) SVM (optimal cost) :  %s",sum(test.errorp2[1,2]+test.errorp2[2,1])/sum(test.errorp2))
train.pred2 = predict(tune.out$best.model,newdata = train)
train.error2 = table(predict = train.pred2,truth = train$Purchase)
test.pred2 = predict(tune.out$best.model,newdata = test)
test.error2 = table(predict = test.pred2,truth = test$Purchase)
train.error2
sprintf("Overall training misclassification rate for linear SVM (Optimal cost):  %s",sum(train.error2[1,2]+train.error2[2,1])/sum(train.error2))
test.error2
sprintf("Overall test misclassification rate for linear SVM (Optimal cost):  %s",sum(test.error2[1,2]+test.error2[2,1])/sum(test.error2))
?model.matrix
model.matrix(as.factor(c(1,2,3,4)))
ff <- log(Volume) ~ log(Height) + log(Girth)
utils::str(m <- model.frame(ff, trees))
mat <- model.matrix(ff, m)
View(train)
fit.svcp = svm(train$Purchase~.,data = train[,-1], kernel = "polynomial",degree = 2, cost = 0.05, scale = T)
summary(fit.svcp)
train.predp = predict(fit.svcp,train)
train.errorp = table(predict = train.predp,truth = train$Purchase)
test.predp = predict(fit.svcp,test)
test.errorp = table(predict = test.predp,truth = test$Purchase)
train.errorp
sprintf("Overall training misclassification rate for polynomial kernel (D=2) SVM:  %s",sum(train.errorp[1,2]+train.errorp[2,1])/sum(train.errorp))
test.errorp
sprintf("Overall test misclassification rate for polynomial kernel (D=2) SVM:  %s",sum(test.errorp[1,2]+test.errorp[2,1])/sum(test.errorp))
SuppressMassages(library(stats))
SuppressMessages(library(stats))
suppressMessages(library(stats))
?kmeans
suppressMessages(library(stats))
set.seed (2)
x=matrix (rnorm (50*2) , ncol =2)
x[1:25 ,1]=x[1:25 ,1]+3
x[1:25 ,2]=x[1:25 ,2]-4
km.out =kmeans (x,2, nstart =2)
plot(x, col =(km.out$cluster +1) , main="K-Means Clustering
Results with K=2", xlab ="", ylab="", pch =20, cex =2)
set.seed (2)
x=matrix (rnorm (50*2) , ncol =2)
x[1:25 ,1]=x[1:25 ,1]+3
x[1:25 ,2]=x[1:25 ,2]-4
km.out =kmeans (x,2, nstart =20)
plot(x, col =(km.out$cluster +1) , main="K-Means Clustering
Results with K=2", xlab ="", ylab="", pch =20, cex =2)
suppressMessages(library(stats))
set.seed (2)
x=matrix (rnorm (50*2) , ncol =2)
x[1:25 ,1]=x[1:25 ,1]+3
x[1:25 ,2]=x[1:25 ,2]-4
km.out =kmeans (x,2, nstart =1)
plot(x, col =(km.out$cluster +1) , main="K-Means Clustering
Results with K=2", xlab ="", ylab="", pch =20, cex =2)
set.seed (2)
x=matrix (rnorm (50*2) , ncol =2)
x[1:25 ,1]=x[1:25 ,1]+3
x[1:25 ,2]=x[1:25 ,2]-4
km.out =kmeans (x,2, nstart =20)
plot(x, col =(km.out$cluster +1) , main="K-Means Clustering
Results with K=2", xlab ="", ylab="", pch =20, cex =2)
suppressMessages(library(stats))
set.seed (1)
x=matrix (rnorm (50*2) , ncol =2)
x[1:25 ,1]=x[1:25 ,1]+3
x[1:25 ,2]=x[1:25 ,2]-4
km.out =kmeans (x,2, nstart =1)
plot(x, col =(km.out$cluster +1) , main="K-Means Clustering
Results with K=2", xlab ="", ylab="", pch =20, cex =2)
set.seed (1)
x=matrix (rnorm (50*2) , ncol =2)
x[1:25 ,1]=x[1:25 ,1]+3
x[1:25 ,2]=x[1:25 ,2]-4
km.out =kmeans (x,2, nstart =20)
plot(x, col =(km.out$cluster +1) , main="K-Means Clustering
Results with K=2", xlab ="", ylab="", pch =20, cex =2)
suppressMessages(library(stats))
set.seed (1)
x=matrix (rnorm (50*2) , ncol =2)
x[1:25 ,1]=x[1:25 ,1]+3
x[1:25 ,2]=x[1:25 ,2]+3
km.out =kmeans (x,2, nstart =1)
plot(x, col =(km.out$cluster +1) , main="K-Means Clustering
Results with K=2", xlab ="", ylab="", pch =20, cex =2)
suppressMessages(library(stats))
set.seed (1)
x=matrix (rnorm (50*2) , ncol =2)
x[1:25 ,1]=x[1:25 ,1]+3
x[1:25 ,2]=x[1:25 ,2]+3
km.out =kmeans (x,2, nstart =1)
plot(x, col =(km.out$cluster +1) , main="K-Means Clustering
Results with K=2", xlab ="", ylab="", pch =20, cex =2)
set.seed (1)
x=matrix (rnorm (50*2) , ncol =2)
x[1:25 ,1]=x[1:25 ,1]+3
x[1:25 ,2]=x[1:25 ,2]+3
km.out =kmeans (x,2, nstart =20)
plot(x, col =(km.out$cluster +1) , main="K-Means Clustering
Results with K=2", xlab ="", ylab="", pch =20, cex =2)
View(x)
set.seed (1)
x=matrix (rnorm (50*2) , ncol =1)
# x[1:25 ,1]=x[1:25 ,1]+3
# x[1:25 ,2]=x[1:25 ,2]+3
km.out =kmeans (x,2, nstart =20)
plot(x, col =(km.out$cluster +1) , main="K-Means Clustering
Results with K=2", xlab ="", ylab="", pch =20, cex =2)
suppressMessages(library(stats))
set.seed (1)
x=matrix (rnorm (50*2) , ncol =1)
# x[1:25 ,1]=x[1:25 ,1]+3
# x[1:25 ,2]=x[1:25 ,2]+3
km.out =kmeans (x,2, nstart =1)
plot(x, col =(km.out$cluster +1) , main="K-Means Clustering
Results with K=2", xlab ="", ylab="", pch =20, cex =2)
suppressMessages(library(stats))
set.seed (1)
x=matrix (rnorm (50*2) , ncol =2)
# x[1:25 ,1]=x[1:25 ,1]+3
# x[1:25 ,2]=x[1:25 ,2]+3
km.out =kmeans (x,2, nstart =1)
plot(x, col =(km.out$cluster +1) , main="K-Means Clustering
Results with K=2", xlab ="", ylab="", pch =20, cex =2)
suppressMessages(library(stats))
set.seed (1)
x=matrix (rnorm (100*2) , ncol =2)
# x[1:25 ,1]=x[1:25 ,1]+3
# x[1:25 ,2]=x[1:25 ,2]+3
km.out =kmeans (x,2, nstart =1)
plot(x, col =(km.out$cluster +1) , main="K-Means Clustering
Results with K=2", xlab ="", ylab="", pch =20, cex =2)
set.seed (1)
x=matrix (rnorm (100*2) , ncol =2)
# x[1:25 ,1]=x[1:25 ,1]+3
# x[1:25 ,2]=x[1:25 ,2]+3
km.out =kmeans (x,2, nstart =20)
plot(x, col =(km.out$cluster +1) , main="K-Means Clustering
Results with K=2", xlab ="", ylab="", pch =20, cex =2)
suppressMessages(library(stats))
set.seed (1)
x=matrix (rnorm (100*2) , ncol =2)
# x[1:25 ,1]=x[1:25 ,1]+3
# x[1:25 ,2]=x[1:25 ,2]+3
km.out1 =kmeans (x,2, nstart =1)
plot(x, col =(km.out1$cluster +1) , main="K-Means Clustering
Results with K=2", xlab ="", ylab="", pch =20, cex =2)
set.seed (1)
x=matrix (rnorm (100*2) , ncol =2)
# x[1:25 ,1]=x[1:25 ,1]+3
# x[1:25 ,2]=x[1:25 ,2]+3
km.out2 =kmeans (x,2, nstart =20)
plot(x, col =(km.out2$cluster +1) , main="K-Means Clustering
Results with K=2", xlab ="", ylab="", pch =20, cex =2)
suppressMessages(library(stats))
set.seed (1)
x=matrix (rnorm (50*2) , ncol =2)
# x[1:25 ,1]=x[1:25 ,1]+3
# x[1:25 ,2]=x[1:25 ,2]+3
km.out1 =kmeans (x,2, nstart =1)
plot(x, col =(km.out1$cluster +1) , main="K-Means Clustering
Results with K=2", xlab ="", ylab="", pch =20, cex =2)
set.seed (1)
x=matrix (rnorm (50*2) , ncol =2)
# x[1:25 ,1]=x[1:25 ,1]+3
# x[1:25 ,2]=x[1:25 ,2]+3
km.out2 =kmeans (x,2, nstart =20)
plot(x, col =(km.out2$cluster +1) , main="K-Means Clustering
Results with K=2", xlab ="", ylab="", pch =20, cex =2)
suppressMessages(library(stats))
set.seed (1)
x=matrix (rnorm (50*2) , ncol =2)
# x[1:25 ,1]=x[1:25 ,1]+3
# x[1:25 ,2]=x[1:25 ,2]+3
km.out1 =kmeans (x,2, nstart =0)
suppressMessages(library(stats))
set.seed (1)
x=matrix (rnorm (50*2) , ncol =2)
# x[1:25 ,1]=x[1:25 ,1]+3
# x[1:25 ,2]=x[1:25 ,2]+3
km.out1 =kmeans (x,2, nstart =1)
plot(x, col =(km.out1$cluster +1) , main="K-Means Clustering
Results with K=2", xlab ="", ylab="", pch =20, cex =2)
?plot
suppressMessages(library(stats))
set.seed (1)
x=matrix (rnorm (50*2) , ncol =2)
# x[1:25 ,1]=x[1:25 ,1]+3
# x[1:25 ,2]=x[1:25 ,2]+3
km.out1 =kmeans (x,2, nstart =1)
plot(x= x[,1],y = x[,2], col =(km.out1$cluster +1) , main="K-Means Clustering
Results with K=2", xlab ="", ylab="", pch =20, cex =2)
set.seed (10)
x=matrix (rnorm (50*2) , ncol =2)
# x[1:25 ,1]=x[1:25 ,1]+3
# x[1:25 ,2]=x[1:25 ,2]+3
km.out2 =kmeans (x,2, nstart =20)
plot(x, col =(km.out2$cluster +1) , main="K-Means Clustering
Results with K=2", xlab ="", ylab="", pch =20, cex =2)
suppressMessages(library(stats))
set.seed (10)
x=matrix (rnorm (50*2) , ncol =2)
# x[1:25 ,1]=x[1:25 ,1]+3
# x[1:25 ,2]=x[1:25 ,2]+3
km.out1 =kmeans (x,2, nstart =1)
plot(x= x[,1],y = x[,2], col =(km.out1$cluster +1) , main="K-Means Clustering
Results with K=2", xlab ="", ylab="", pch =20, cex =2)
setwd("C:/Users/pdutta03/Google Drive/Q-Winter2017/STATS 290/Project/Final_submission_4explore/stat290project-master")
library(devtools)
install.packages("dplyr")
install("statgeophy")
library("statgeophy")
?statgeophy
shinyShearRange
shinyShearRange()
shinyCompositeShear
shinyCompositeShear()
?generateData
